{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229acc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import *\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "from .utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc2e6da",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "label_array = {\n",
    "    0: \"top\",\n",
    "    1: \"trouser\",\n",
    "    2: \"pullover\",\n",
    "    3: \"dress\",\n",
    "    4: \"coat\",\n",
    "    5: \"sandal\",\n",
    "    6: \"shirt\",\n",
    "    7: \"sneaker\",\n",
    "    8: \"bag\",\n",
    "    9: \"ankle_boot\",\n",
    "}\n",
    "\n",
    "\n",
    "def return_shape(tes):\n",
    "    print(tes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1c612a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def print_shapes(arr):\n",
    "    for i in arr:\n",
    "        return_shape(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965a8369",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def visualize_image(features, res_path):\n",
    "    for i in range(25):\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(features[i].reshape(28, 28), cmap=\"gray\")\n",
    "    # plt.show()\n",
    "    plt.savefig(f\"{res_path}/dataset_image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2a1ccf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_data(main_path, subset=None):\n",
    "    df_train = pd.read_csv(str(main_path / \"fashion-mnist_train.csv\"))\n",
    "    df_test = pd.read_csv(str(main_path / \"fashion-mnist_test.csv\"))\n",
    "    print(df_train.head(5))\n",
    "\n",
    "    if subset != None and subset > 0:\n",
    "        df_train = df_train.head(subset)\n",
    "        df_test = df_test.head(subset)\n",
    "\n",
    "    train_features, val_features, train_labels, val_labels = train_test_split(\n",
    "        df_train.drop(\"label\", axis=1), df_train[\"label\"], test_size=0.2\n",
    "    )\n",
    "    test_features, test_labels = df_test.drop(\"label\", axis=1), df_test[\"label\"]\n",
    "    (\n",
    "        train_features,\n",
    "        val_features,\n",
    "        train_labels,\n",
    "        val_labels,\n",
    "        test_features,\n",
    "        test_labels,\n",
    "    ) = (\n",
    "        train_features.to_numpy(),\n",
    "        val_features.to_numpy(),\n",
    "        train_labels.to_numpy(),\n",
    "        val_labels.to_numpy(),\n",
    "        test_features.to_numpy(),\n",
    "        test_labels.to_numpy(),\n",
    "    )\n",
    "    print(\"[INFO] DONE LOADING DATA\")\n",
    "    return (\n",
    "        train_features,\n",
    "        val_features,\n",
    "        train_labels,\n",
    "        val_labels,\n",
    "        test_features,\n",
    "        test_labels,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df539deb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# ML PIPELINE\n",
    "\n",
    "\n",
    "def return_process(im):\n",
    "    list_of_procs = [np.flip, lambda x: x * 2, lambda x: x ** 2]\n",
    "    return random.choice(list_of_procs)(\n",
    "        im\n",
    "    )  # TODO :add probability and every other transform\n",
    "\n",
    "\n",
    "def preprocess_skeleton(array, labels, disable=False, sequential=True):\n",
    "    # TODO : Append results to the array instead of replacing it\n",
    "    # TODO : Use the labels to make sure it is correct after you append something\n",
    "    if disable == True:\n",
    "        return array, labels\n",
    "    else:\n",
    "        if sequential == True:\n",
    "            # array = parallel(return_process, arr=array)\n",
    "            array = np.array([return_process(x) for x in array])\n",
    "            return array, labels\n",
    "        else:\n",
    "            array = parallel(return_process, arr=array)\n",
    "            return array, labels\n",
    "\n",
    "\n",
    "def dimensionality_reduction(X, X_test, method=\"pca\"):\n",
    "    if method == \"pca\":\n",
    "        pca = PCA(0.85)\n",
    "        pca.fit(X)\n",
    "        X = pca.transform(X)\n",
    "        X_test = pca.transform(X_test)\n",
    "\n",
    "\n",
    "def train_and_predict(\n",
    "    train_features,\n",
    "    test_features,\n",
    "    train_labels,\n",
    "    test_labels,\n",
    "    val_features,\n",
    "    val_labels,\n",
    "    model,\n",
    "    metrics,\n",
    "    res_path,\n",
    "    reduce_dims=None,\n",
    "    folds=10,\n",
    "):\n",
    "    # scale data\n",
    "    train_features = np.concatenate(\n",
    "        (train_features, test_features)\n",
    "    )  # combining both here because k-fold will split them again into parts\n",
    "    train_labels = np.concatenate((train_labels, test_labels))\n",
    "    X = StandardScaler().fit_transform(train_features)\n",
    "    X_test = StandardScaler().fit_transform(val_features)  # essentially validation set\n",
    "\n",
    "    # preprocess_step\n",
    "    X, train_labels = preprocess_skeleton(\n",
    "        X, train_labels, disable=False\n",
    "    )  # enable for processing\n",
    "\n",
    "    visualize_image(X, res_path)\n",
    "    if reduce_dims != None:\n",
    "        dimensionality_reduction(X, X_test, reduce_dims)\n",
    "\n",
    "    # k -fold\n",
    "    dict_results = {x.__name__: [] for x in metrics}\n",
    "\n",
    "    for metric in metrics:\n",
    "        if metric.__name__ in [\"precision_score\", \"f1_score\", \"recall_score\"]:\n",
    "            dict_results[metric.__name__] = np.mean(\n",
    "                cross_val_score(\n",
    "                    model,\n",
    "                    X,\n",
    "                    train_labels,\n",
    "                    scoring=make_scorer(metric, average=\"micro\"),\n",
    "                    cv=folds,\n",
    "                    n_jobs=8,\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            dict_results[metric.__name__] = np.mean(\n",
    "                cross_val_score(\n",
    "                    model,\n",
    "                    X,\n",
    "                    train_labels,\n",
    "                    scoring=make_scorer(metric),\n",
    "                    cv=folds,\n",
    "                    n_jobs=8,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    print(dict_results)\n",
    "\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plot_confusion_matrix(model.fit(X, train_labels), X_test, val_labels)\n",
    "    plt.savefig(f\"{res_path}/confusion_{str(model)}.png\")\n",
    "\n",
    "    return dict_results\n",
    "\n",
    "\n",
    "def multi_model_run(\n",
    "    train_features,\n",
    "    test_features,\n",
    "    train_labels,\n",
    "    test_labels,\n",
    "    model_list,\n",
    "    ##################\n",
    "    model_parameters,\n",
    "    ##################\n",
    "    reduce_dims,\n",
    "    metrics,\n",
    "    res_path,\n",
    "    val_features,\n",
    "    val_labels,\n",
    "    folds=10,\n",
    "):\n",
    "    num_params_per_model = []\n",
    "    for params in model_parameters:\n",
    "        num_params_per_model.append(len(params))\n",
    "\n",
    "    ############################\n",
    "    # TODO: VALIDATION: Figure out how to access list of models and edit params - Tumi\n",
    "    ############################\n",
    "\n",
    "    final_dict_results = {}\n",
    "    for model in tqdm(model_list):\n",
    "        final_dict_results[str(model)] = train_and_predict(\n",
    "            train_features=train_features,\n",
    "            test_features=test_features,\n",
    "            train_labels=train_labels,\n",
    "            test_labels=test_labels,\n",
    "            val_features=val_features,\n",
    "            val_labels=val_labels,\n",
    "            model=model,\n",
    "            reduce_dims=reduce_dims,\n",
    "            metrics=metrics,\n",
    "            res_path=res_path,\n",
    "            folds=folds,\n",
    "        )\n",
    "    print(final_dict_results)\n",
    "    df = pd.DataFrame.from_dict(final_dict_results)\n",
    "    df.to_csv(f\"{res_path}/outputs.csv\", mode=\"a\")\n",
    "    return final_dict_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0ac498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
