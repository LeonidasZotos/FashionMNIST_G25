{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0d1fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from scipy import ndimage\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import *\n",
    "from sklearn.metrics import (plot_confusion_matrix, plot_det_curve,\n",
    "                             plot_roc_curve)\n",
    "from sklearn.model_selection import (GridSearchCV, KFold, cross_val_score,\n",
    "                                     train_test_split)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from tqdm import tqdm\n",
    "\n",
    "from .utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9126e5b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "label_array = {\n",
    "    0: \"top\",\n",
    "    1: \"trouser\",\n",
    "    2: \"pullover\",\n",
    "    3: \"dress\",\n",
    "    4: \"coat\",\n",
    "    5: \"sandal\",\n",
    "    6: \"shirt\",\n",
    "    7: \"sneaker\",\n",
    "    8: \"bag\",\n",
    "    9: \"ankle_boot\",\n",
    "}\n",
    "\n",
    "\n",
    "def return_shape(tes):\n",
    "    print(tes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184791e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def print_shapes(arr):\n",
    "    for i in arr:\n",
    "        return_shape(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b4088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Used to visualise the images\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def visualize_image(features, res_path):\n",
    "    for i in range(25):\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(features[i].reshape(28, 28), cmap=\"gray\")\n",
    "    # plt.show()\n",
    "    plt.savefig(f\"{res_path}/dataset_image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628c21b6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Load the training and testing data from the fashion-mnist data set\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def load_data(main_path, subset=None):\n",
    "    df_train = pd.read_csv(str(main_path / \"fashion-mnist_train.csv\"))\n",
    "    df_test = pd.read_csv(str(main_path / \"fashion-mnist_test.csv\"))\n",
    "    print(df_train.head(5))\n",
    "\n",
    "    # If a subset has been defined and is valid\n",
    "    if subset != None and subset > 0:\n",
    "        df_train = df_train.head(subset)\n",
    "        df_test = df_test.head(subset)\n",
    "\n",
    "    train_features, val_features, train_labels, val_labels = train_test_split(\n",
    "        df_train.drop(\"label\", axis=1), df_train[\"label\"], test_size=0.2\n",
    "    )\n",
    "\n",
    "    test_features, test_labels = df_test.drop(\"label\", axis=1), df_test[\"label\"]\n",
    "    (\n",
    "        train_features,\n",
    "        val_features,\n",
    "        train_labels,\n",
    "        val_labels,\n",
    "        test_features,\n",
    "        test_labels,\n",
    "    ) = (\n",
    "        train_features.to_numpy(),\n",
    "        val_features.to_numpy(),\n",
    "        train_labels.to_numpy(),\n",
    "        val_labels.to_numpy(),\n",
    "        test_features.to_numpy(),\n",
    "        test_labels.to_numpy(),\n",
    "    )\n",
    "\n",
    "    print(\"[INFO] DONE LOADING DATA\")\n",
    "    return (\n",
    "        train_features,\n",
    "        val_features,\n",
    "        train_labels,\n",
    "        val_labels,\n",
    "        test_features,\n",
    "        test_labels,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735eaefa",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# ML PIPELINE\n",
    "\n",
    "\n",
    "def flip(x):  # Horizontal flip\n",
    "    # 28*28\n",
    "    x = np.reshape(x, (28, 28))\n",
    "    x = np.fliplr(x)\n",
    "    x = x.flatten()  # Return to 1d array\n",
    "    return x\n",
    "\n",
    "\n",
    "def gaussian_filter(x):\n",
    "    # sigma=1 seems to give best results\n",
    "    return ndimage.gaussian_filter(x, sigma=1)\n",
    "\n",
    "\n",
    "def maxf(x):\n",
    "    return ndimage.maximum_filter(x, 2)\n",
    "\n",
    "\n",
    "def convolveOutline(x):\n",
    "    weights = [-1, 4, -1]  # Outlining\n",
    "    try:\n",
    "        x = ndimage.convolve(x, weights)\n",
    "    except RuntimeError:\n",
    "        x = x[:, :, None]\n",
    "        x = ndimage.convolve(x, weights)\n",
    "    return x\n",
    "\n",
    "\n",
    "def convolveSharpen(x):\n",
    "    weights = [0, 2, 0]  # Sharpening\n",
    "    try:\n",
    "        x = ndimage.convolve(x, weights)\n",
    "    except RuntimeError:\n",
    "        x = x[:, :, None]\n",
    "        x = ndimage.convolve(x, weights)\n",
    "    return x\n",
    "\n",
    "\n",
    "def visualize_transforms(features, res_path):\n",
    "    from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "    im1 = features[0]\n",
    "    im2 = flip(im1).reshape(28, 28)\n",
    "    im3 = convolveOutline(im1).reshape(28, 28)\n",
    "    im4 = convolveSharpen(im1).reshape(28, 28)\n",
    "    im5 = gaussian_filter(im1).reshape(28, 28)\n",
    "    im6 = maxf(im1).reshape(28, 28)\n",
    "\n",
    "    _, axs = plt.subplots(3, 2)\n",
    "    axs = axs.flatten()\n",
    "    names = [\"original\", \"flip\", \"outline\", \"sharpen\", \"gaussian\", \"maximum\"]\n",
    "    i = 0\n",
    "    for img, ax in zip([im1.reshape(28, 28), im2, im3, im4, im5, im6], axs):\n",
    "        ax.imshow(img, cmap=\"gray\")\n",
    "        ax.axes.xaxis.set_visible(False)\n",
    "        ax.axes.yaxis.set_visible(False)\n",
    "        ax.set_title(names[i])\n",
    "        i += 1\n",
    "\n",
    "    plt.savefig(f\"{res_path}/transform_image.png\")\n",
    "\n",
    "\n",
    "def return_process(im):\n",
    "    list_of_transforms = [\n",
    "        lambda x: x,\n",
    "        flip,\n",
    "        gaussian_filter,\n",
    "        convolveOutline,\n",
    "        convolveSharpen,\n",
    "    ]  # first returns the original (no transform)\n",
    "    # \"weights\" determines probability to choose each transformation\n",
    "    chosenTransform = random.choices(list_of_transforms, weights=(0, 1, 1, 1, 1))\n",
    "    return chosenTransform[0](im)\n",
    "\n",
    "\n",
    "def preprocess_skeleton(array, labels, disable=False, sequential=True):\n",
    "    if disable == True:\n",
    "        return array, labels\n",
    "    else:\n",
    "        if sequential == True:\n",
    "            # array = parallel(return_process, arr=array)\n",
    "            percentToTransform = (\n",
    "                0.5  # use 0.5 to transform 50% of the data and append to the end\n",
    "            )\n",
    "            originalLength = len(array)\n",
    "            numberOfTransforms = math.floor(len(array) * percentToTransform)\n",
    "\n",
    "            for i in range(0, numberOfTransforms):\n",
    "                chosenIndex = random.randrange(originalLength)\n",
    "                transformedImage = return_process(array[chosenIndex])\n",
    "                # add transformed image to set\n",
    "                array = np.vstack((array, transformedImage))\n",
    "                # also copy the label\n",
    "                labels = np.append(labels, labels[chosenIndex])\n",
    "\n",
    "            return array, labels\n",
    "        else:\n",
    "            array = parallel(return_process, arr=array)\n",
    "            return array, labels\n",
    "\n",
    "\n",
    "def dimensionality_reduction(X, X_test, method=\"pca\"):\n",
    "    if method == \"pca\":\n",
    "        pca = PCA(0.85)\n",
    "        pca.fit(X)\n",
    "        X = pca.transform(X)\n",
    "        X_test = pca.transform(X_test)\n",
    "\n",
    "\n",
    "def train_and_predict(\n",
    "    train_features,\n",
    "    test_features,\n",
    "    train_labels,\n",
    "    test_labels,\n",
    "    val_features,\n",
    "    val_labels,\n",
    "    model,\n",
    "    metrics,\n",
    "    res_path,\n",
    "    reduce_dims=None,\n",
    "    folds=10,\n",
    "):\n",
    "    # scale data\n",
    "    train_features = np.concatenate(\n",
    "        (train_features, test_features)\n",
    "    )  # combining both here because k-fold will split them again into parts\n",
    "    train_labels = np.concatenate((train_labels, test_labels))\n",
    "    X = StandardScaler().fit_transform(train_features)\n",
    "    X_test = StandardScaler().fit_transform(val_features)  # essentially validation set\n",
    "\n",
    "    # preprocess_step\n",
    "    X, train_labels = preprocess_skeleton(\n",
    "        X,\n",
    "        train_labels,\n",
    "    )  # enable for processing\n",
    "\n",
    "    visualize_image(X, res_path)\n",
    "    visualize_transforms(X, res_path)\n",
    "    if reduce_dims != None:\n",
    "        dimensionality_reduction(X, X_test, reduce_dims)\n",
    "\n",
    "    # k -fold\n",
    "    dict_results = {x.__name__: [] for x in metrics}\n",
    "\n",
    "    for metric in metrics:\n",
    "        if metric.__name__ in [\"precision_score\", \"f1_score\", \"recall_score\"]:\n",
    "            dict_results[metric.__name__] = np.mean(\n",
    "                cross_val_score(\n",
    "                    model,\n",
    "                    X,\n",
    "                    train_labels,\n",
    "                    scoring=make_scorer(metric, average=\"micro\"),\n",
    "                    cv=folds,\n",
    "                    n_jobs=8,\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            dict_results[metric.__name__] = np.mean(\n",
    "                cross_val_score(\n",
    "                    model,\n",
    "                    X,\n",
    "                    train_labels,\n",
    "                    scoring=make_scorer(metric),\n",
    "                    cv=folds,\n",
    "                    n_jobs=8,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    print(dict_results)\n",
    "\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plot_confusion_matrix(model.fit(X, train_labels), X_test, val_labels)\n",
    "    plt.savefig(f\"{res_path}/confusion_{str(model)}.png\")\n",
    "\n",
    "    return dict_results\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Method used to do validation of the models in the model list over a predefined list of parameters\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def validation_stage(\n",
    "    model_list,\n",
    "    model_parameters,\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    test_features,\n",
    "    test_labels,\n",
    "):\n",
    "    # Perform the Validation using Sklearn GridSearch\n",
    "\n",
    "    model_best_params = []\n",
    "\n",
    "    print(\"[INFO] Performing Validation\")\n",
    "    for i in range(len(tqdm(model_list))):\n",
    "        print(\"[VALIDATION] %s model being validated\\n\" % (model_list[i]))\n",
    "        grid = GridSearchCV(\n",
    "            model_list[i](), model_parameters[i], verbose=3, refit=True, n_jobs=12\n",
    "        )\n",
    "        grid.fit(train_features, train_labels)\n",
    "\n",
    "        model_best_params.append(grid.best_params_)\n",
    "\n",
    "        grid_predictions = grid.predict(test_features)\n",
    "\n",
    "        print(classification_report(test_labels, grid_predictions))\n",
    "        print(\n",
    "            \"[VALIDATION] %s model validation completed\\n\"\n",
    "            \"==========================================\\n\" % (model_list[i])\n",
    "        )\n",
    "\n",
    "    print(\"[INFO] Validation Stage Completed\")\n",
    "\n",
    "    return model_best_params\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Method used to run multiple models for comparison\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def multi_model_run(\n",
    "    train_features,\n",
    "    test_features,\n",
    "    train_labels,\n",
    "    test_labels,\n",
    "    model_list,\n",
    "    model_parameters,\n",
    "    reduce_dims,\n",
    "    metrics,\n",
    "    res_path,\n",
    "    val_features,\n",
    "    val_labels,\n",
    "    folds=10,\n",
    "):\n",
    "    # Determine the best parameters for each of the models defined in model list and from the list of model parameters\n",
    "    model_best_params = validation_stage(\n",
    "        model_list,\n",
    "        model_parameters,\n",
    "        train_features,\n",
    "        train_labels,\n",
    "        test_features,\n",
    "        test_labels,\n",
    "    )\n",
    "\n",
    "    # Create instances of the models in the model lists while making use of the best\n",
    "    #   parameters attained in the validation stage\n",
    "    print(\"[INFO] Applying Best Parameters to Models\")\n",
    "    for i in range(len(tqdm(model_list))):\n",
    "        model_list[i] = model_list[i](**model_best_params[i])\n",
    "\n",
    "    print(\"[INFO] Training and Testing Optimised Models\")\n",
    "\n",
    "    final_dict_results = {}\n",
    "    results = Parallel(n_jobs=2)(\n",
    "        delayed(train_and_predict)(\n",
    "            train_features,\n",
    "            test_features,\n",
    "            train_labels,\n",
    "            test_labels,\n",
    "            val_features,\n",
    "            val_labels,\n",
    "            m,\n",
    "            metrics,\n",
    "            res_path,\n",
    "            reduce_dims,\n",
    "            folds,\n",
    "        )\n",
    "        for m in tqdm(model_list)\n",
    "    )\n",
    "    print(\"[INFO] Models Done Running\\n[INFO] Compiling Results\")\n",
    "\n",
    "    final_dict_results = {\n",
    "        str(model_list[i]): results[i] for i in range(len(model_list))\n",
    "    }\n",
    "\n",
    "    # print(final_dict_results)\n",
    "    df = pd.DataFrame.from_dict(final_dict_results)\n",
    "    df.to_csv(f\"{res_path}/outputs.csv\", mode=\"a\")\n",
    "    return final_dict_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7415b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
