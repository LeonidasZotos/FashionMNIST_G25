{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b090c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6f893e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def return_shape(tes):\n",
    "    print(tes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ff5ba9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def print_shapes(arr):\n",
    "    for i in arr:\n",
    "        return_shape(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12123d0d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def visualize_image(features):\n",
    "    for i in range(25):\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(features[i].reshape(28, 28))\n",
    "    # plt.show()\n",
    "    plt.savefig(\"../dataset_image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86011fe2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_data(main_path, subset=None):\n",
    "    df_train = pd.read_csv(str(main_path / \"fashion-mnist_train.csv\"))\n",
    "    df_test = pd.read_csv(str(main_path / \"fashion-mnist_test.csv\"))\n",
    "    print(df_train.head(5))\n",
    "\n",
    "    if subset != None and subset > 0:\n",
    "        df_train = df_train.head(subset)\n",
    "        df_test = df_test.head(subset)\n",
    "\n",
    "    train_features, val_features, train_labels, val_labels = train_test_split(\n",
    "        df_train.drop(\"label\", axis=1), df_train[\"label\"], test_size=0.2\n",
    "    )\n",
    "    test_features, test_labels = df_test.drop(\"label\", axis=1), df_test[\"label\"]\n",
    "    (\n",
    "        train_features,\n",
    "        val_features,\n",
    "        train_labels,\n",
    "        val_labels,\n",
    "        test_features,\n",
    "        test_labels,\n",
    "    ) = (\n",
    "        train_features.to_numpy(),\n",
    "        val_features.to_numpy(),\n",
    "        train_labels.to_numpy(),\n",
    "        val_labels.to_numpy(),\n",
    "        test_features.to_numpy(),\n",
    "        test_labels.to_numpy(),\n",
    "    )\n",
    "    print(\"[INFO] DONE LOADING DATA\")\n",
    "    return (\n",
    "        train_features,\n",
    "        val_features,\n",
    "        train_labels,\n",
    "        val_labels,\n",
    "        test_features,\n",
    "        test_labels,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e616805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML PIPELINE\n",
    "\n",
    "\n",
    "def preproces_skeleton(array, process=np.flip):\n",
    "    # currently does for all images in array\n",
    "    # TODO : Add results to the array\n",
    "    if process == None:\n",
    "        return array\n",
    "    else:\n",
    "        for i in range(array.shape[0]):\n",
    "            array[i] = process(array[i])\n",
    "        return array\n",
    "\n",
    "\n",
    "def train_and_predict(\n",
    "    train_features,\n",
    "    test_features,\n",
    "    train_labels,\n",
    "    test_labels,\n",
    "    model,\n",
    "    metrics,\n",
    "    normalize=True,\n",
    "):\n",
    "    # scale data\n",
    "    X = StandardScaler().fit_transform(train_features)\n",
    "    X_test = StandardScaler().fit_transform(test_features)\n",
    "\n",
    "    # preprocess_step\n",
    "    X = preproces_skeleton(X, None)\n",
    "\n",
    "    # fit model\n",
    "    model.fit(X, train_labels)\n",
    "    y1 = test_labels\n",
    "    y2 = model.predict(X_test)\n",
    "\n",
    "    # metrics\n",
    "    if type(metrics) != list:\n",
    "        metrics = [metrics]\n",
    "\n",
    "    dict_results = {}\n",
    "    for metric in metrics:\n",
    "        try:\n",
    "            dict_results[metric.__name__] = metric(y1, y2)\n",
    "        except ValueError:\n",
    "            dict_results[metric.__name__] = metric(y1, y2, average=\"macro\")\n",
    "\n",
    "    print(dict_results)\n",
    "    return dict_results\n",
    "\n",
    "\n",
    "def multi_model_run(\n",
    "    train_features, test_features, train_labels, test_labels, model_list, metrics\n",
    "):\n",
    "    final_dict_results = {}\n",
    "    for model in tqdm(model_list):\n",
    "        final_dict_results[str(model)] = train_and_predict(\n",
    "            train_features=train_features,\n",
    "            test_features=test_features,\n",
    "            train_labels=train_labels,\n",
    "            test_labels=test_labels,\n",
    "            model=model,\n",
    "            metrics=metrics,\n",
    "        )\n",
    "    print(final_dict_results)\n",
    "    df = pd.DataFrame.from_dict(final_dict_results)\n",
    "    df.to_csv(\"outputs.csv\", mode=\"a\")\n",
    "    return final_dict_results"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
