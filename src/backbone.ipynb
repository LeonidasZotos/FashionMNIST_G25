{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8378bad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import *\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import (GridSearchCV, KFold, cross_val_score,\n",
    "                                     train_test_split)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "from .utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f8675f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "label_array = {\n",
    "    0: \"top\",\n",
    "    1: \"trouser\",\n",
    "    2: \"pullover\",\n",
    "    3: \"dress\",\n",
    "    4: \"coat\",\n",
    "    5: \"sandal\",\n",
    "    6: \"shirt\",\n",
    "    7: \"sneaker\",\n",
    "    8: \"bag\",\n",
    "    9: \"ankle_boot\",\n",
    "}\n",
    "\n",
    "\n",
    "def return_shape(tes):\n",
    "    print(tes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa4a6e4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def print_shapes(arr):\n",
    "    for i in arr:\n",
    "        return_shape(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fd0303",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def visualize_image(features, res_path):\n",
    "    for i in range(25):\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.imshow(features[i].reshape(28, 28), cmap=\"gray\")\n",
    "    # plt.show()\n",
    "    plt.savefig(f\"{res_path}/dataset_image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec4bb77",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_data(main_path, subset=None):\n",
    "    df_train = pd.read_csv(str(main_path / \"fashion-mnist_train.csv\"))\n",
    "    df_test = pd.read_csv(str(main_path / \"fashion-mnist_test.csv\"))\n",
    "    print(df_train.head(5))\n",
    "\n",
    "    if subset != None and subset > 0:\n",
    "        df_train = df_train.head(subset)\n",
    "        df_test = df_test.head(subset)\n",
    "\n",
    "    train_features, val_features, train_labels, val_labels = train_test_split(\n",
    "        df_train.drop(\"label\", axis=1), df_train[\"label\"], test_size=0.2\n",
    "    )\n",
    "    test_features, test_labels = df_test.drop(\"label\", axis=1), df_test[\"label\"]\n",
    "    (\n",
    "        train_features,\n",
    "        val_features,\n",
    "        train_labels,\n",
    "        val_labels,\n",
    "        test_features,\n",
    "        test_labels,\n",
    "    ) = (\n",
    "        train_features.to_numpy(),\n",
    "        val_features.to_numpy(),\n",
    "        train_labels.to_numpy(),\n",
    "        val_labels.to_numpy(),\n",
    "        test_features.to_numpy(),\n",
    "        test_labels.to_numpy(),\n",
    "    )\n",
    "    print(\"[INFO] DONE LOADING DATA\")\n",
    "    return (\n",
    "        train_features,\n",
    "        val_features,\n",
    "        train_labels,\n",
    "        val_labels,\n",
    "        test_features,\n",
    "        test_labels,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cffd35a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# ML PIPELINE\n",
    "\n",
    "\n",
    "def return_process(im):\n",
    "    list_of_procs = [np.flip, lambda x: x * 2, lambda x: x ** 2]\n",
    "    return random.choice(list_of_procs)(\n",
    "        im\n",
    "    )  # TODO :add probability and every other transform\n",
    "\n",
    "\n",
    "def preprocess_skeleton(array, labels, disable=False, sequential=True):\n",
    "    # TODO : Append results to the array instead of replacing it\n",
    "    # TODO : Use the labels to make sure it is correct after you append something\n",
    "    if disable == True:\n",
    "        return array, labels\n",
    "    else:\n",
    "        if sequential == True:\n",
    "            # array = parallel(return_process, arr=array)\n",
    "            array = np.array([return_process(x) for x in array])\n",
    "            return array, labels\n",
    "        else:\n",
    "            array = parallel(return_process, arr=array)\n",
    "            return array, labels\n",
    "\n",
    "\n",
    "def dimensionality_reduction(X, X_test, method=\"pca\"):\n",
    "    if method == \"pca\":\n",
    "        pca = PCA(0.85)\n",
    "        pca.fit(X)\n",
    "        X = pca.transform(X)\n",
    "        X_test = pca.transform(X_test)\n",
    "\n",
    "\n",
    "def train_and_predict(\n",
    "    train_features,\n",
    "    test_features,\n",
    "    train_labels,\n",
    "    test_labels,\n",
    "    val_features,\n",
    "    val_labels,\n",
    "    model,\n",
    "    metrics,\n",
    "    res_path,\n",
    "    reduce_dims=None,\n",
    "    folds=10,\n",
    "):\n",
    "    # scale data\n",
    "    train_features = np.concatenate(\n",
    "        (train_features, test_features)\n",
    "    )  # combining both here because k-fold will split them again into parts\n",
    "    train_labels = np.concatenate((train_labels, test_labels))\n",
    "    X = StandardScaler().fit_transform(train_features)\n",
    "    X_test = StandardScaler().fit_transform(val_features)  # essentially validation set\n",
    "\n",
    "    # preprocess_step\n",
    "    X, train_labels = preprocess_skeleton(\n",
    "        X, train_labels, disable=False\n",
    "    )  # enable for processing\n",
    "\n",
    "    visualize_image(X, res_path)\n",
    "    if reduce_dims != None:\n",
    "        dimensionality_reduction(X, X_test, reduce_dims)\n",
    "\n",
    "    # k -fold\n",
    "    dict_results = {x.__name__: [] for x in metrics}\n",
    "\n",
    "    for metric in metrics:\n",
    "        if metric.__name__ in [\"precision_score\", \"f1_score\", \"recall_score\"]:\n",
    "            dict_results[metric.__name__] = np.mean(\n",
    "                cross_val_score(\n",
    "                    model,\n",
    "                    X,\n",
    "                    train_labels,\n",
    "                    scoring=make_scorer(metric, average=\"micro\"),\n",
    "                    cv=folds,\n",
    "                    n_jobs=8,\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            dict_results[metric.__name__] = np.mean(\n",
    "                cross_val_score(\n",
    "                    model,\n",
    "                    X,\n",
    "                    train_labels,\n",
    "                    scoring=make_scorer(metric),\n",
    "                    cv=folds,\n",
    "                    n_jobs=8,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    print(dict_results)\n",
    "\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    plot_confusion_matrix(model.fit(X, train_labels), X_test, val_labels)\n",
    "    plt.savefig(f\"{res_path}/confusion_{str(model)}.png\")\n",
    "\n",
    "    return dict_results\n",
    "\n",
    "\n",
    "def validation_stage(\n",
    "    model_list,\n",
    "    model_parameters,\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    test_features,\n",
    "    test_labels,\n",
    "):\n",
    "    # Perform the Validation using Sklearn GridSearch\n",
    "    model_best_params = []\n",
    "    print(\"[INFO] Performing Validation\")\n",
    "    for i in range(len(tqdm(model_list))):\n",
    "        print(\"[VALIDATION] %s model being validated\\n\" % (model_list[i]))\n",
    "        grid = GridSearchCV(\n",
    "            model_list[i](), model_parameters[i], verbose=1, refit=True, n_jobs=12\n",
    "        )\n",
    "        grid.fit(train_features, train_labels)\n",
    "\n",
    "        model_best_params.append(grid.best_params_)\n",
    "        # model_best_params[model_list[i]] = grid.best_params_\n",
    "        grid_predictions = grid.predict(test_features)\n",
    "\n",
    "        print(classification_report(test_labels, grid_predictions))\n",
    "        print(\n",
    "            \"[VALIDATION] %s model validation completed\\n\"\n",
    "            \"==========================================\\n\" % (model_list[i])\n",
    "        )\n",
    "\n",
    "    print(\"[INFO] Validation Stage Completed\")\n",
    "\n",
    "    return model_best_params\n",
    "\n",
    "\n",
    "def multi_model_run(\n",
    "    train_features,\n",
    "    test_features,\n",
    "    train_labels,\n",
    "    test_labels,\n",
    "    model_list,\n",
    "    ##################\n",
    "    model_parameters,\n",
    "    ##################\n",
    "    reduce_dims,\n",
    "    metrics,\n",
    "    res_path,\n",
    "    val_features,\n",
    "    val_labels,\n",
    "    folds=10,\n",
    "):\n",
    "\n",
    "    ############################\n",
    "    model_best_params = validation_stage(\n",
    "        model_list,\n",
    "        model_parameters,\n",
    "        train_features,\n",
    "        train_labels,\n",
    "        test_features,\n",
    "        test_labels,\n",
    "    )\n",
    "\n",
    "    print(\"[INFO] Applying Best Parameters to Models\")\n",
    "    for i in range(len(tqdm(model_list))):\n",
    "        model_list[i] = model_list[i](**model_best_params[i])\n",
    "\n",
    "    print(\"[INFO] Training and Testing Optimised Models\")\n",
    "    ############################\n",
    "\n",
    "    final_dict_results = {}\n",
    "    results = Parallel(n_jobs=2)(\n",
    "        delayed(train_and_predict)(\n",
    "            train_features,\n",
    "            test_features,\n",
    "            train_labels,\n",
    "            test_labels,\n",
    "            val_features,\n",
    "            val_labels,\n",
    "            m,\n",
    "            metrics,\n",
    "            res_path,\n",
    "            reduce_dims,\n",
    "            folds,\n",
    "        )\n",
    "        for m in tqdm(model_list)\n",
    "    )\n",
    "\n",
    "    final_dict_results = {\n",
    "        str(model_list[i]): results[i] for i in range(len(model_list))\n",
    "    }\n",
    "    # print(final_dict_results)\n",
    "    df = pd.DataFrame.from_dict(final_dict_results)\n",
    "    df.to_csv(f\"{res_path}/outputs.csv\", mode=\"a\")\n",
    "    return final_dict_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a380a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
